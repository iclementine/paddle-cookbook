# 使用动态图

首先我们来说一下动态图这个概念。如果是从 dynet 的角度来理解，动态图的意义在于为可以每个 example 创建不同的图。而静态图的意义是是把图创建出来之后就固定住，类似于 `graph.compile()`, 然后把图构建和图运行分开处理。当然这二者之间并没有截然的区别，比如说 tensorflow 中，也可以在图执行一部分之后，扩展图。而 dynet 中也可以固定同一个 Expression, 只是不断复写它的值。对于 dynet 来说，都是声明式 (declarative) 编程范式，只是静态声明还是动态声明的差别。

而 Paddle 所说的动态图概念上指的是 imperative，亦即命令式编程。执行是即时的。这一点类似 pytorch, 而 pytorch 的使用，是接近 numpy 的，比如说 `c = a + b`，那么你可以立即得到结果 `c`。而对于动态声明式的 dynet，你需要显示地要求 `eval`，而对于静态声明式的 tensorflow, 而是需要 `session.run(feed_dict, fetch)`。

下面我们来比较一下，几种不同的库下的写法吧。

## tensorflow

```pyton
import tensorflow as tf

# graph assembly
a = tf.get_variable("a", shape=(2, 3))
b = tf.get_variable("b", shape=(2, 3))
c = a + b
d = a * b

init = tf.global_variables_initializer()

with tf.Session() as sess:
	sess.run(init)
	[c_np, d_np] = sess.run([c, d])

print(c_np)
# [[ 0.2730686  -0.13175237  0.0929454 ]
#  [-0.3833214   0.18598104  0.38718557]]
print(d_np)
# [[-0.5206642  -0.6174296  -0.4701086 ]
#  [ 0.02292797 -0.11577368 -0.04476679]]
```

### 计算图

以上有一个明显的间隔，`tf.Session()`，这里标示着会话的开启，从这里可以分开两个的部分，前面的部分是图的组建，`with tf.Session() as sess` 后面的部分则是图的执行的部分。接下来还有几个 **心理** 上的概念需要理解。`a`, `b`, `c`, `d`, `init` 都是符号，实际的计算尚未发生。并且通过这些操作，你在一张空白的图上添加了几个节点和连接的边，这个图就是所谓的计算图，而且这也是深度学习框架实现中的一个很重要的概念，**但是，你们说的那个图，它在哪里呢？**我们添加的那些节点，到哪里去了呢? 

按着 tensorflow 的编程范例学习而来，你可能不会去思考这样的问题，就像说，我们的世界，在哪里呢？它就在那里，本来就在，甚至自然到你不会发问。在你 `import tensorflow as tf` 的时候，默认就帮你开了一个图，而且你也不用显式地指明，我要把节点创建在哪个图上，默认就创建在那个图上。

但是，如果当到了后来因为某些原因，你要知道你正在组建哪个图，那么怎么办呢？也不是没有办法啦。`cg = tf.get_default_graph()` 就可以做到。而且这个方法只是一个 `getter` 而不会创建一个新的图。

既然说到这里了，那么我就是要 **显式** 地指定把节点创建到哪个图上呢（因为这使我感到 *explicit*）? `tf.get_variable` 方法是否允许我传一个参数指定哪个图呢？很遗憾，并不能。你只能通过上下文管理器来做这个事情。比如说，如果你喜欢显式地组建计算图,那么你可以这样。

```python
cg = tf.Graph()

with cg.as_default():
  a = tf.get_variable("a", shape=(2, 3))
  b = tf.get_variable("b", shape=(2, 3))
  c = a + b
  d = a * b
```

通过这么做，你可以心安理得。并且可以通过 `assert a.graph is cg` 来确认这一点。

### 会话

那么接下来还有一个问题，既然你说要组建图，又要运行图，那么我运行的是哪个图呢？嗯。确实是的，`tf.Session` 对象的创建确实和一个图联系在一起，表明要执行的是哪个图。那么，`Session` 这个概念是不是和 `Graph` 重叠呢？并不，因为还有一个 `ExecutionEngine` （执行引擎）的概念，图是可执行的内容，而执行引擎则是说明应该如何执行。执行引擎的概念，留作后话。同样，和上面类似，如果你喜欢显式地把计算图和会话之间的联系写明白，那么你可以这样写。

```python
with tf.Session(graph=cg) as sess:
	sess.run(init)
	[c_np, d_np] = sess.run([c, d])
```

这样是不是舒服多了，这样子概念上的东西就清晰了，而少了很多 magic。

### 图和函数

接下来我们来看一看 `sess.run` 方法。其实我们传的这个参数是 `fetches` 参数，亦即需要取回的节点，这是需要的位置参数，意图很明显，`sess.run([c, d])` 这样看起来就像是说我需要运行这两个节点，并且顺带地把不得不运行的节点也运行了。然后 `sess.run` 的返回值就和 `fetches` 意义对应，你要取回什么，session 就给你取回什么。听起来很精妙？你仿佛获得了一个可以随便生成函数的工具，比如 `sess.run([c, d])` 就相当于一个返回 `c, d` 的函数，`sess.run([c])` 就相当于一个返回 c 的函数。

但是你可能在心里想，这么一大堆概念是干什么哟？并且觉得不就是下面这一段吗？

```python
import numpy as np

# emmm... graph assembly
def func():
  a = np.random.randn(2, 3)
  b = np.random.randn(2, 3)
  c = a + b
  d = a * b
  return c, d

# emmm... graph execution
c_np, d_np = func()
```

其实，从某种意义上这就是这样子。当定义上述函数 `func` 的时候，里面的 `a` 和 `b` 也没有被创建。也都是个符号，`c` 和 `d` 也没有被计算出来，只有调用 `func` 的时候，才计算出来了。蛤，你跟我说图构建，图执行？不就是定义函数和执行函数吗？是的，概念上没有什么差别。

嗯，还有一点差别，那就是对于 `session.run` 来说，`fetches` 就相当于函数的返回值，并且你可以自由取用。这样， `sess.run(fetches)` 就相当于` func` 调用。

不过，其实你也可以恶心一下自己，把上面那一套符合正常逻辑的东西写得很 `graphic`。比如如下的代码，你也可以做到类似的功能。

```python
import numpy as np
from contextlib import contextmanager

def func():
  a = np.random.randn(2, 3)
  b = np.random.randn(2, 3)
  c = a + b
  d = a * b
  return locals()

@contextmanager
def session(func):
  results = func()
  try:
    def get_variable(fetches):
      return [results[name] for name in fetches]
    yield get_variable
  finally:
    del get_variable
    
with session(func) as sess:
  c_np, d_np = sess(["c", "d"])

c_np
# array([[-0.80174464, -1.43231564,  1.42780047],
#        [-0.81094556, -0.23120116, -0.08001297]])
d_np
# array([[ 0.65352793, -0.88984254, -1.44032034],
#        [ 0.95641549, -2.10568344,  0.78615194]])
```

上述的形式是不是很丧心病狂？一般人是不会这么写代码的啦。但是通过这个方式，确实可以把正常代码弄得和静态图一样的，一言难尽。并且，把函数 `func` 定义成返回所有的局部变量 `locals()`，使得我们可以窥探其中所有的局部变量。在使用了上下文管理器的时候，我们获得窥探全部局部变量的机会，并且通过定义了 `get_variable` 闭包来实现按需取用。要看什么内部变量，就传递这个变量的名字。这样就和 `Graph`, `Session` 之类的概念，具有了形式上的相似性。

这么说来，既然用简单的，易于理解的代码就可以实现和曲折麻烦的代码一样的功能，为何有要去写后者这种风格呢？当然，静态图有部署上的考虑，我这里只是为了做对比。

对于定义一个函数，结果就是定义了一个函数。对应于图，那就是在图上创建了一些节点和连接。后者看起来更加具体 （**concret**） 是吗？看起来图上被写了一些东西了，而定义一个函数，并不会有什么副作用。似乎是的，那是因为图是一种脱离了某种具体的编程语言的东西，虽然我们必须写一些代码用于构建图。而对于 python 的运行时来说，写下定义 `func` 的那些代码，就是定义了一个函数，只是说里面那些变量都是 python 运行时可以识别的符号。

让我们把这个对应关系写得更加明晰：

| 函数                   | 计算图         |
| ---------------------- | -------------- |
| 定义函数               | 组建计算图     |
| 函数定义里面的局部变量 | 计算图上的节点 |
| 函数定义里面的运算     | 计算图上的算子 |

那么问题来了，在一种编程语言里，用代码定义一个函数就能够实现的功能，在计算图里面怎么做？我们目前其实还是通过编程语言去做，比如 `a = tf.get_variable("a", (2, 3))` 就在图上创建一个节点，但是这一行代码需要直接执行才能创建一个节点，如果这一句是在 python 函数的定义里面，那么就要运行才能在图上创建一个节点。

这也构成了我在概念上的一个难题，为了定义图上的一个 `a + b` 这样的运算逻辑，你需要真的把它加到图上去。而不是，**有代码还不够吗，你还要一个脱离编程语言的逻辑？** 对于静态图而言，确实如此，它就是要一个脱离具体编程语言的逻辑。这个东西往往通过一些方式保存下来，比如说 Protobuf，然后用于分析和执行。 

那为什么不能像定义一个函数一样直接组计算图呢？可能的答案是，**你需要一个计算图的运行时才能这么做**。不然就始终是间接地去实现。此为后话，暂且挂起。

接下来可以看看一些其他的方法怎么做这件事。

## pytorch

上面提到我们不能直接像定义函数那样组建计算图。而必须要自己手动一点一点地去把节点和连接装上去。是不是感觉很间接，很难受？不过问题不大，我们可以先来写个理想的伪代码。

```python
def add(a: Variable, b:Variable) -> Variable:
  c = a + b
  return c
```

这样就定义一个加法。然后使用的时候就直接 `c = add(a, b)` 就直接得到想要的值，岂不美哉？我根本不关心是不是有一个计算图，上面定义了两个节点和这两个节点的和 （sum）节点。我也不会在调用完 `add` 之后还妄图去访问其内部的局部变量 `c`。也就是说 `add` 函数执行的时候这一部分图构建出来，计算完了，c 就消失了，很自然合理吧，这样就和普通函数没有区别了（当然为了反向传播维持变量的事情是后事，以后再讲）。然后，声明式也就成了命令式。

对于 pytorch 来说，是典型的命令式编程，非常接近 numpy, 和上面相应的代码可以这么写。你不会感觉到计算图的存在，也不会感觉到 Session 的开启和关闭。事实上并没有 Session 的概念，计算图也是只有一个。

```python
import torch
a = torch.randn(2, 3)
b = torch.randn(2, 3)
c = a + b
```



## dynet

dynet 所说的动态图是动态声明 (dynamic declarative) 式计算图。

对于 dynet 框架来说，相应的代码是。

```python
import dynet as dy
a = dy.random_normal((2, 3))
b = dy.random_normal((2, 3))
c = a + b
c_np = c.npvalue()
# array([[ 0.88382947,  0.76931781,  1.18417835],
#        [-0.84307122,  0.242764  ,  0.36432958]])
```

非常直接。尽管 dynet 也有 Graph 的概念，但是你可以不断扩展图，也可以在必要的时候求值，然后继续扩展。dynet 的 计算图是一个单子，任何时候只有一个计算图对象存在。概念上接近于 tensorflow 的 Graph 打开了 Session 的时候。不存在一个显式的开启会话的过程，随时都可以计算，随时也都可以扩展图。计算图也是用完就丢弃重开 `dy.renew_cg()`, 与此相应的是，dynet 的概念中，参数存在于计算图之外。此是后话，之后再介绍）。和 pytorch 不同的一点在于 dynet 的开一张新的计算图这个操作是手动的，而 pytorch 则自动把用完的节点丢弃，因此自始至终只有一张计算图。（相比来说，从理论上看，dynet 的方法仍然允许静态图的图优化方法，而 pytorch 则不允许，因为它默认 eager execution。）

一个值得注意的是，上述的 `c = a + b`，仅仅是构建图，直到需要才计算。这样，可以通过使用表达式 (Expression) 组合，不断地把图扩大，直到触发求值。比如，`c = a + b; d = c + e, ...`  ，然后可以对一个表达式使用 `eval()` 方法。比如，`e.eval()` 才进行计算。因此这就仍然还是声明式。这就是 dynet 的典型范式。（注：dynet 和 pytorch 的写法事实上可以很相似，而且 dynet 本身也可以通过一个开关直接切换成命令式，语法上不必有任何更改）

上述的代码对于命令式也没有任何问题，区别就是 `c` 是一个符号还是一个真实的计算结果。而对于声明式或者是带有懒惰求值机制的库而言，符号和真实的值之间是可以相互运算的，比如说 Eigen 库就是通过表达式的方式扩大计算图，然后进行懒惰求值，所以语法糖才能这么甜啊。这并不是什么新鲜的事情，并不是神经网络计框架独有的东西。



总的来说，在动态声明式编程或者命令式编程范式的框架里，session 的概念消失了，图组建和图执行是混合在一起进行的，没有明显的二段式分离。因此需要的功能可以写在函数里面，这个函数被执行的时候，需要的图就会被构建出来。而且计算图的组建与执行和客户端语言更加紧密地联系在一起，你会更加感觉你是在写代码，而不是在拼水管以及开闸放水。

# 逻辑与计算图

逻辑，这是一个很有趣的概念，在人们试图用言语去描述和编程相关的事情的时候，常常会报出来这个词。用于描述一件事应该如何做，就好比剧本一样，规定了应该如何演戏。我们也常用 “函数的逻辑” 来指称函数定义中的一系列操作。

前文曾经做过定义函数和定义计算图的对比。那么我们来谈谈逻辑的描述。对于函数来说，很简单，代码就是描述，写出来就可以了。而因为用编程语言去操作计算图时候属于间接操作，我们需要真正执行一些代码才能组建计算图，把计算逻辑描述写到图上去。

那么接下来，我们来回答另一个问题？**如何保存这个逻辑呢？**，对于函数，可能我们都会一脸茫然啊。对于解释型语言，把代码存下来不就行了？你还想要什么逻辑？到要执行时候解释器按着代码来不就可以了？对于编译型语言来说，直接编译也就可以了。不然呢？

那为什么还需要费心思在出了保存源代码和保存编译结果两种方式之外，用另外的途径去保存计算图上呢？问题就是出在这里。**为了一次编写，到处运行，各种环境运行**，比如说 python 写的计算逻辑，到时候要到没有 python 的环境下运行，诸如此类。但是又不想用另一种语言重新写一遍，就必须考虑这样的事情。

一般怎么做呢？其实还是通过 protobuf 把图的节点和节点的依赖关系保存下来，这个用于执行。

【未完待续...】

## 





